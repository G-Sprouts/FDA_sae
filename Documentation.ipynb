{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sorter (files)\n",
    "\n",
    "* Convert all values in 'prod_ai' column to string values, and then separate non-nan values for class mapping.\n",
    "\n",
    "* The 'prod_ai' (product active ingredient) column is used for this and downstream functions because unlike the brand name, an active ingredient/generic name may have a shared suffix with other medications, which makes the mapping functions computationally efficient.\n",
    "\n",
    "* Append each sorted dataframe as a list to allow mapping function iterations to run separately and maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dfs = [ ]\n",
    "missing_dfs = [ ]\n",
    "new_files = [ ]\n",
    "positives = [ ]\n",
    "inds = [ ]\n",
    "\n",
    "def sorter(files):\n",
    "        \n",
    "    for drug_file in files:\n",
    "\n",
    "        drug_file.prod_ai = drug_file.prod_ai.astype(str)\n",
    "        drug_file.prod_ai = drug_file.prod_ai.map(lambda x: x.replace('.', ''))\n",
    "\n",
    "        indices = drug_file[drug_file.prod_ai != 'nan'].index\n",
    "        nan_indices = drug_file[drug_file.prod_ai == 'nan'].index\n",
    "\n",
    "        present = drug_file.prod_ai.loc[indices]\n",
    "        absent = drug_file.prod_ai.loc[nan_indices]\n",
    "        \n",
    "        class_df.drugname = present\n",
    "\n",
    "        missing_df.drugname = drug_file.drugname.loc[nan_indices]\n",
    "        missing_df.generic = absent\n",
    "\n",
    "\n",
    "        class_dfs.append([class_df])\n",
    "        missing_dfs.append([missing_df])\n",
    "        new_files.append([drug_file])\n",
    "        positives.append([present])\n",
    "        inds.append([indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_1(class_df,p,i)\n",
    "\n",
    "* First round of mapping logic.\n",
    "\n",
    "* At completion of iteration, separate mapped drug names and indices from drug names and indices where no class was mapped, then send the unmapped entries into the next mapping function.\n",
    "     * Instead of sending each original dataframe through the full mapping logic, which is extremely computationally expensive, only send the original through a small portion of the logic and separate the entries that returned nan. That smaller dataframe is then sent through the next mapping function, which has the same .loc separater steps, and then send an even smaller dataframe through the third round of logic.\n",
    "     * This cascade-style mapping proves to be very efficient, especially when handling 1.5+ million observations per dataframe.\n",
    "\n",
    "* Create local variable for mapped entries, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_1(class_df,p,i):\n",
    "\n",
    "    for x,y in zip(p,i):\n",
    "    \n",
    "        (mapping logic)...\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "    lead_df = class_df[class_df.class_id != 'nan']\n",
    "    df_2 = class_df[class_df.class_id == 'nan']\n",
    "    \n",
    "    idx = df_2.index\n",
    "    drugs = df_2.drugname\n",
    "    \n",
    "    return map_2(df_2,drugs,idx,lead_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_2(class_df,drugs,idx,lead_df)\n",
    "\n",
    "* Second round of mapping logic.\n",
    "* See map_1 for explanation...\n",
    "* Create local variable of concatenated dataframes, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_2(class_df,drugs,idx,lead_df):\n",
    "\n",
    "    for x,y in zip(drugs,idx):\n",
    "    \n",
    "        (mapping logic)...\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "            \n",
    "    df_2 = class_df[class_df.class_id != 'nan']\n",
    "    df_3 = class_df[class_df.class_id == 'nan']\n",
    "    final_df = pd.concat([lead_df, df_2])\n",
    "    \n",
    "    idx = df_3.index\n",
    "    drugs = df_3.drugname\n",
    "    \n",
    "    return map_3(df_3,drugs,idx, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_3(class_df,drugs,idx, final_df)\n",
    "\n",
    "* Third round of mapping logic.\n",
    "* see map_1 for explanation...\n",
    "* Create local variable of concatenated dataframes and a dataframe of all entries that did not meet any of the mapping logic, then append each into their respective global list to examine once functions are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dfs = []\n",
    "miss_dfs = []\n",
    "\n",
    "def map_3(class_df,drugs,idx, final_df):\n",
    "    \n",
    "    for x,y in zip(drugs,idx):\n",
    "        \n",
    "        (mapping logic)...\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "    miss_df = class_df[class_df.class_id == 'nan']\n",
    "    class_df = class_df[class_df.class_id != 'nan']\n",
    "    final_df = pd.concat([final_df, class_df])\n",
    "    \n",
    "    final_dfs.append(final_df)\n",
    "    miss_dfs.append(miss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *file_merge(saved_dfs, files)\n",
    "\n",
    "* Read in the mapped and saved dataframes as well as the original files, select which columns from the original files you'd wish to merge with the mapped dataframes, and then contatenate them creating a custom table to analyze.\n",
    "* Append new dataframes to global list variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additions = []\n",
    "custom_dfs = []\n",
    "\n",
    "def file_merge(saved_dfs, files):\n",
    "    \n",
    "    for f,z in zip(saved_dfs, files):\n",
    "        indices = f.index\n",
    "        kept = z[['primaryid', 'caseid', 'dose_form', 'dechal', 'rechal']].loc[indices]\n",
    "        additions.append(kept)\n",
    "        \n",
    "    for f,a in zip(saved_dfs, additions):\n",
    "        new = pd.concat([f,a], axis=1)\n",
    "        custom_dfs.append(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *reacs_map(reacs)\n",
    "* Identify unique primaryids from reaction file(s) to create new DataFrame. then append all corresponding 'Preferred Term' reaction codes to a list, finally iterate through that list to join codes together as a single row in new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_list = []\n",
    "\n",
    "def reacs_map(reacs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ids = reacs.primaryid.unique()\n",
    "    reacs_df = pd.DataFrame(ids, columns=(['primaryid']))    \n",
    "    reacs_df['pt'] = 'nan'\n",
    "        \n",
    "    for x in ids:\n",
    "        df = reacs[reacs.primaryid==x]\n",
    "        pt_list.append(df.pt.values)\n",
    "    for i,a in enumerate(pt_list):\n",
    "        reacs_df.pt.loc[i] = ' , '.join(a)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print((end_time - start_time) / 60 / 60)\n",
    "    print('Completed. Check your defined variable for output')\n",
    "    return reacs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *outs_map(outs) \n",
    "* Identify unique primaryids from outcome file(s) to create new DataFrame, then append all corresponding outcome codes to a list. Finally, iterate through the list and join all codes in a single row in new DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_code_list = []\n",
    "\n",
    "def outs_map(outs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ids = outs.primaryid.unique()\n",
    "    outs_df = pd.DataFrame(ids, columns=(['primaryid']))    \n",
    "    outs_df['pt'] = 'nan'\n",
    "    \n",
    "    \n",
    "    for x in ids:\n",
    "        df = outs[outs.primaryid==x]\n",
    "        outs_code_list.append(df.outc_cod.values)\n",
    "    for i,a in enumerate(outs_code_list):\n",
    "        outs_df.pt.loc[i] = ' , '.join(a)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print((end_time - start_time) / 60 / 60)\n",
    "    print('Completed. Check your defined variable for output')\n",
    "    return outs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *file_merge(saved_dfs, drug_files, df1, df2)\n",
    "* First create a variable representing the indicies from the previously mapped DataFrame and use this variable to locate the primaryid and caseid of each mapped drug from within the original drug file. Append this DataFrame copy to a list. \n",
    "* Next, join the saved_df with the newly appended DataFrame copy and create two new rows for the 'Preferred Term' reactions and 'outcome codes', which will be filled with the values obtained from 'reacs_map' and 'outs_map'.\n",
    "* Finally, enumerate the 'reacs' and 'outs' DataFrames and merge their values with the 'saved_df' by matching on primaryid. Append this final DataFrame to a list for further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additions = []\n",
    "custom_dfs = []\n",
    "\n",
    "def file_merge(saved_dfs, drug_files, df1, df2):\n",
    "    for sf,df in zip(saved_dfs, drug_files):\n",
    "        indices = f.orig_idx\n",
    "        kept = df[['primaryid','caseid']].loc[indices]\n",
    "        additions.append(kept)\n",
    "        \n",
    "    for f,a,rf,of in zip(saved_dfs, additions, df1, df2):\n",
    "        primaries = kept.primaryid\n",
    "        new = f.join([a.set_index(f.index)])\n",
    "        new['pt'] = 'nan'\n",
    "        new['outc_cod'] = 'nan'\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        for i,x in enumerate(rf.primaryid):\n",
    "            for j,y in enumerate(new.primaryid):\n",
    "                if x == y:\n",
    "                    new.pt.loc[j] = rf.pt.loc[i]\n",
    "        for i,x in enumerate(of.primaryid):\n",
    "            for j,y in enumerate(new.primaryid):\n",
    "                if x == y:\n",
    "                    new.outc_cod.loc[j] = of.outc_cod.loc[i]\n",
    "\n",
    "    custom_dfs.append(new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
