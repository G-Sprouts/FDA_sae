{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use these functions in succession to create a custom dataset. If you would like, clone this repository and adjust the functions to include the features of your liking. \n",
    "\n",
    "## For example, under *file_merge(), adjust the \"kept\" variable to reflect whichever columns you prefer from the original dataset to be included within your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sorter (files)\n",
    "\n",
    "* Convert all values in 'prod_ai' column to string values, and then separate non-nan values for class mapping.\n",
    "\n",
    "* The 'prod_ai' (product active ingredient) column is used for this and downstream functions because unlike the brand name, an active ingredient/generic name may have a shared suffix with other medications, which makes the mapping functions computationally efficient.\n",
    "\n",
    "* Append each sorted dataframe as a list to allow mapping function iterations to run separately and maintain data integrity.\n",
    "* The top_indices argument slices a pd.value_counts() output to return the indices of the top pd.value_counts() of the prod_ai column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dfs = []\n",
    "missing_dfs = []\n",
    "positives = []\n",
    "indices = []\n",
    "grouped_list = []\n",
    "\n",
    "def sorter(drug_file_path,top_indices=None):\n",
    "    \n",
    "    grouped_list = []\n",
    "    drug_file = pd.read_csv(drug_file_path, delimiter='$')\n",
    "\n",
    "    drug_file.prod_ai = drug_file.prod_ai.astype(str)\n",
    "    drug_file.prod_ai = drug_file.prod_ai.map(lambda x: x.replace('.', ''))\n",
    "    \n",
    "    index = drug_file[drug_file.prod_ai != 'nan'].index\n",
    "    nan_index = drug_file[drug_file.prod_ai == 'nan'].index\n",
    "    \n",
    "    present = drug_file.prod_ai.loc[index]\n",
    "    present_primaryids = drug_file.primaryid.loc[index]\n",
    "    \n",
    "    absent = drug_file.prod_ai.loc[nan_index]\n",
    "    absent_primaryids = drug_file.primaryid.loc[nan_index]\n",
    "    \n",
    "    class_df = pd.DataFrame(columns=['primaryid', 'prod_ai', 'class_id', 'class', 'indication'])\n",
    "    missing_df = pd.DataFrame(columns=['primaryid', 'prod_ai', 'class_id', 'class', 'indication'])\n",
    "    \n",
    "    class_df.primaryid = present_primaryids\n",
    "    class_df.prod_ai = present\n",
    "    \n",
    "    missing_df = absent_primaryids\n",
    "    missing_df.prod_ai = absent\n",
    "    \n",
    "    if top_indices == None:\n",
    "        top_indices = -1\n",
    "    grouped_list = [class_df[class_df.loc[:,'prod_ai'] == x].index for x in class_df.prod_ai.value_counts()[:top_indices].index]\n",
    "    class_df = class_df.loc[itertools.chain.from_iterable(grouped_list)]\n",
    "    \n",
    "    \n",
    "    class_dfs.append([class_df])\n",
    "    missing_dfs.append([missing_df])\n",
    "    positives.append([present])\n",
    "    indices.append([index])\n",
    "    \n",
    "    print('Check \"class_dfs\", \"missing_dfs\", \"positives\" and \"indices\" for output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_1(class_df,array_split)\n",
    "\n",
    "* First round of mapping logic.\n",
    "* The array_split parameter is passed through each round of logic and is used within the final round of mapping (map_5). This integer value tells a final if/else statement whether the split dataframe has its full parts within a list, in which case it will concatenate the parts into the size of the original dataframe (minus values that did not meet mapping logic conditions). See 'for loop' example at bottom.\n",
    "* At completion of iteration, separate mapped drug names and indices from drug names and indices where no class was mapped, then send the unmapped entries into the next mapping function.\n",
    "     * Instead of sending each original dataframe through the full mapping logic, which is extremely computationally expensive, split the dataframe with numpy function np.array_split(), and send each section through the logic, and separate the entries that returned nan. That smaller dataframe is then sent through the next mapping function, which has the same .loc separater steps, and then send an even smaller dataframe through the third round of logic and so on.\n",
    "     * This cascade-style mapping proves to be very efficient, especially when handling 1.5+ million observations per dataframe.\n",
    "\n",
    "* Create local variable for mapped entries, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_1(class_df, array_split):\n",
    "\n",
    "    for x,y in zip(class_df.prod_ai,class_df.index):\n",
    "    \n",
    "        (mapping logic)...\n",
    "        \n",
    "    \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "    lead_df = class_df[class_df.class_id != 'nan']\n",
    "    df_2 = class_df[class_df.class_id == 'nan']\n",
    "    \n",
    "    idx = df_2.index\n",
    "    drugs = df_2.prod_ai\n",
    "   \n",
    "    return map_2(df_2,drugs,idx,lead_df,array_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_2(class_df,drugs,idx,lead_df,array_split)\n",
    "\n",
    "* Second round of mapping logic.\n",
    "* See map_1 for explanation...\n",
    "* Create local variable of concatenated dataframes, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_2(class_df,drugs,idx,lead_df,array_split):\n",
    "\n",
    "    for x,y in zip(drugs,idx):\n",
    "    \n",
    "        (mapping logic)...\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "            \n",
    "    df_2 = class_df[class_df.class_id != 'nan']\n",
    "    df_3 = class_df[class_df.class_id == 'nan']\n",
    "    final_df = pd.concat([lead_df, df_2])\n",
    "    \n",
    "    idx = df_3.index\n",
    "    drugs = df_3.prod_ai\n",
    "    \n",
    "    return map_3(df_3,drugs,idx, final_df,array_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_3(class_df,drugs,idx, final_df,array_split)\n",
    "\n",
    "* Third round of mapping logic.\n",
    "* see map_1 for explanation...\n",
    "* Create local variable of concatenated dataframes, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dfs = []\n",
    "miss_dfs = []\n",
    "\n",
    "def map_3(class_df,drugs,idx,final_df,array_split):\n",
    "    \n",
    "    for x,y in zip(drugs,idx):\n",
    "        \n",
    "        (mapping logic)...\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "            \n",
    "    df_3 = class_df[class_df.class_id != 'nan']\n",
    "    df_4 = class_df[class_df.class_id == 'nan']\n",
    "    final_df = pd.concat([final_df, df_3])\n",
    "    \n",
    "    idx = df_4.index\n",
    "    drugs = df_4.prod_ai\n",
    "    \n",
    "    return map_4(df_4,drugs,idx,final_df, array_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_4(class_df,drugs,idx,final_df,array_split)\n",
    "* Fourth round of mapping logic.\n",
    "* See map_1 for explanation...\n",
    "* Create local variable of concatenated dataframes, and send that to next function to merge with the next round of mapped entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_4(class_df,drugs,idx,final_df,array_split):\n",
    "    \n",
    "    for x,y in zip(drugs,idx):\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "            \n",
    "    df_4 = class_df[class_df.class_id != 'nan']\n",
    "    df_5 = class_df[class_df.class_id == 'nan']\n",
    "    final_df = pd.concat([final_df, df_4])\n",
    "    \n",
    "    idx = df_5.index\n",
    "    drugs = df_5.prod_ai\n",
    "    \n",
    "    return map_5(df_5,drugs,idx,final_df,array_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map_5(class_df,drugs,idx,final_df,array_split)\n",
    "* Create local variable of concatenated dataframes and a dataframe of all entries that did not meet any of the mapping logic, then append each into their respective global list to examine once functions are completed.\n",
    "* The final if/else statement checks if each section of the original dataframe is present in a list, and then concatenates the list to recreate the original dataframe (minus values that did not meet mapping logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_5(class_df,drugs,idx, final_df,array_split):\n",
    "    \n",
    "    for x,y in zip(drugs,idx):\n",
    "        \n",
    "    class_df.class_id = class_df.class_id.astype(str)\n",
    "    miss_df = class_df[class_df.class_id == 'nan']\n",
    "    class_df = class_df[class_df.class_id != 'nan']\n",
    "    final_df = pd.concat([final_df, class_df])\n",
    "    \n",
    "    global final_storage_list\n",
    "    global final_missing_storage_list\n",
    "    \n",
    "    final_storage_list.append(final_df)\n",
    "    final_missing_storage_list.append(miss_df)\n",
    "    \n",
    "    if len(final_storage_list) == array_split:\n",
    "        final_df = pd.concat(final_storage_list)\n",
    "        final_df = final_df.sort_values(by='primaryid').reset_index(drop=True)\n",
    "        final_dfs.append(final_df)\n",
    "        \n",
    "        miss_df = pd.concat(final_missing_storage_list)\n",
    "        miss_df = miss_df.sort_values(by='primaryid').reset_index(drop=True)\n",
    "        miss_dfs.append(miss_df)\n",
    "\n",
    "        \n",
    "        final_storage_list = []\n",
    "        final_missing_storage = []\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for loop usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in class_dfs:\n",
    "    for class_df in c:\n",
    "        for df in np.array_split(class_df,100):\n",
    "            map_1(df,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *reacs_map(reacs)\n",
    "* Create dictionary of primaryids as key, and pt (Preferred Term) reaction as value(s). \n",
    "* Iterate through dictionary, join values and create dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reacs_map(reactions__file_path):\n",
    "    \n",
    "    reactions_by_id = {}\n",
    "    with open(reactions__file_path) as csvfile:\n",
    "        reacreader = csv.reader(csvfile, delimiter='$')\n",
    "        next(reacreader) \n",
    "         \n",
    "        for row in reacreader:\n",
    "\n",
    "            ptlist = reactions_by_id.get(row[0], [])\n",
    "            ptlist.append(row[2])\n",
    "            reactions_by_id[row[0]] = ptlist\n",
    "\n",
    "        reactions_by_id_list.append(reactions_by_id) \n",
    "        \n",
    "            \n",
    "    reac_df = pd.DataFrame(reactions_by_id.keys(), columns=(['primaryid']))\n",
    "    reac_df = reac_df.sort_values(by='primaryid').set_index('primaryid')\n",
    "    reac_df['pt'] = 'nan'\n",
    "    \n",
    "    for k,v in reactions_by_id.items():\n",
    "        reac_df.loc[k, 'pt'] = ' , '.join(v)\n",
    "    final_reacs.append(reac_df)\n",
    "    \n",
    "    print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *outs_map(outs) \n",
    "* Create dictionary of primaryids as key, and out_code (Outcome Code) as value(s).\n",
    "* Iterate through dictionary, join values, and create dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outs_map(outcomes_file_path):\n",
    "    \n",
    "    outcomes_by_id = {}\n",
    "    with open(outcomes_file_path) as csvfile:\n",
    "        outcreader = csv.reader(csvfile, delimiter='$')\n",
    "        next(outcreader)\n",
    "        \n",
    "        for row in outcreader:\n",
    "            ptlist = outcomes_by_id.get(row[0], [])\n",
    "            ptlist.append(row[2])\n",
    "            outcomes_by_id[row[0]] = ptlist\n",
    "        outcomes_by_id_list.append(outcomes_by_id)\n",
    "        \n",
    "\n",
    "    out_df = pd.DataFrame(outcomes_by_id.keys(), columns=(['primaryid']))\n",
    "    out_df = out_df.sort_values(by='primaryid').set_index('primaryid')\n",
    "    out_df['out_code'] = 'nan'\n",
    "       \n",
    "    for k,v in outcomes_by_id.items():\n",
    "        out_df.loc[k,'out_code'] = ' , '.join(v)\n",
    "    final_outs.append(out_df)\n",
    "    \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *file_merge(saved_dfs, drug_files, df1, df2)\n",
    "* First check if each of the three arguments are string values, if they are, read the path. Otherwise it must be a dataframe object.\n",
    "* Then set index to primaryid for the dataframes, iterate through them, and cast values on matching indices from the reactions and outcomes dataframes to the dataframe retrieved from the class mapping logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_merge(final_class_df_path, final_reac_path, final_out_path):\n",
    "    \n",
    "    if isinstance(final_class_df_path, str):\n",
    "        sd = pd.read_csv(final_class_df_path)  \n",
    "    if isinstance(final_reac_path, str):\n",
    "        fr = pd.read_csv(final_reac_path)\n",
    "    if isinstance(final_out_path, str):\n",
    "        fo = pd.read_csv(final_out_path)\n",
    "    else:\n",
    "        sd = final_class_df_path\n",
    "        fr = final_reac_path\n",
    "        fo = final_out_path\n",
    "    \n",
    "    sd = sd.set_index('primaryid')\n",
    "    fr = fr.set_index('primaryid')\n",
    "    fo = fo.set_index('primaryid')\n",
    "    sd['pt'] = 'nan'\n",
    "    sd['out_code'] = 'nan'\n",
    "    \n",
    "    for x in fr.index:\n",
    "        x = int(x)\n",
    "        \n",
    "        for z in sd.index:\n",
    "            if x == z:\n",
    "                sd.loc[z,'pt'] = fr.loc[x,'pt'] \n",
    "            else:\n",
    "                pass\n",
    "    print('finished with reactions')\n",
    "\n",
    "    for y in fo.index:    \n",
    "        y = int(y)\n",
    "        \n",
    "        for z in sd.index:\n",
    "            if y == z:\n",
    "                sd.loc[z,'out_code'] = fo.loc[y,'out_code']\n",
    "            else:\n",
    "                pass    \n",
    "    print('finished with outcomes')\n",
    "    sd = sd[['primaryid', 'drugname', 'class', 'class_id', 'indication', 'pt', 'out_code']].reset_index()\n",
    "    print('completed')         \n",
    "    custom_dfs.append(sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
